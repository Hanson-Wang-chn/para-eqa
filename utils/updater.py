# utils/updater.py

import logging
import math
import json
import redis
import os

from utils.buffer import Buffer
from utils.vlm_api import VLM_API
from common.redis_client import GROUP_INFO


class Updater:
    def __init__(self, config):
        self.config = config
        self.priority_scheduling = config.get("priority_scheduling", True)
        
        updater_config = self.config.get("updater", {})
        self.enable_cost_estimate = updater_config.get("enable_cost_estimate", False)
        self.enable_reward_estimate = updater_config.get("enable_reward_estimate", False)
        self.enable_urgency = updater_config.get("enable_urgency", True)
        self.enable_scope_type = updater_config.get("enable_scope_type", True)
        self.enable_dependency = updater_config.get("enable_dependency", True)
        
        # Read weight parameters from configuration
        priority_config = config.get("priority", {})
        self.w_urgency = priority_config.get("w_urgency", 1.0)
        self.w_scope = priority_config.get("w_scope", 1.0)
        self.w_cost = priority_config.get("w_cost", -1.0)
        self.w_reward = priority_config.get("w_reward", 1.0)
        self.w_dependency = priority_config.get("w_dependency", 3.0)
        
        # Set up VLM
        self.prompt_updater = config.get("prompt", {}).get("updater", {})
        
        config_vlm = config.get("vlm", {}).get("updater", {})
        model_name = config_vlm.get("model", "qwen/qwen2.5-vl-72b-instruct")
        server = config_vlm.get("server", "openrouter")
        base_url = config_vlm.get("base_url", None)
        api_key = config_vlm.get("api_key", None)
        
        self.vlm = VLM_API(model_name=model_name, server=server, base_url=base_url, api_key=api_key)
        
        # Set up Buffer
        self.buffer = Buffer()
        
        # Store the current highest priority question
        self.highest_priority_question = None
        self.highest_priority_score = float('-inf')


    def add_question(self, question):
        # Finishing Module calls this method to add a question
        self.highest_priority_question = None
        self.highest_priority_score = float('-inf')
        
        # First calculate cost and reward, then calculate dependency, finally status is generated by the calculate_status() method in buffer
        original_questions = self.buffer.get_pending_and_ready_questions()
        all_questions = original_questions.copy()
        all_questions.append(question)
        
        # Calculate reward for all questions
        if self.enable_reward_estimate:
            all_questions = self._get_reward_estimate(all_questions)
        else:
            for q in all_questions:
                q["reward_estimate"] = 0.0
            
        for q in all_questions:
            q["cost_estimate"] = self._get_cost_estimate(q) if self.enable_cost_estimate else 0.0
            
        if len(all_questions) >= 2:
            qid = question["id"]
            new_dependency = self._get_new_dependency(all_questions, qid)
            # new_dependency:
            # {
            #     "depends_on": ["id_1", "id_2", ...],
            #     "required_by": ["id_3", "id_4", ...]
            # }
                
            all_questions = self._merge_dependencies(all_questions, new_dependency, qid)
        
        # Write the updated question list back to buffer
        self.buffer.write_latest_questions(all_questions)
        
        # Rebuild DAG and calculate status
        self.buffer.build_dag()
        self.buffer.calculate_status()
        
        # Update priority scores
        self._update_priority_scores()
    
    
    def complete_question(self, question):
        # Stopping Module calls this method to indicate that a question has been completed
        self.highest_priority_question = None
        self.highest_priority_score = float('-inf')
        
        question_in_progress = self.buffer.get_question_by_id(question["id"])
        if question_in_progress["status"] != "in_progress":
            raise ValueError(f"Question {question_in_progress['id']} is not in progress. Cannot possibly be completed.")
        self.buffer.set_status(question_in_progress["id"], "completed")
        
        # Remove all dependencies related to this question
        other_questions = self.buffer.get_pending_and_ready_questions()
        completed_id = question_in_progress["id"]
        
        # Iterate through each question and remove dependencies on the completed question
        for other_question in other_questions:
            if completed_id in other_question["dependency"]:
                other_question["dependency"].remove(completed_id)
        
        # Recalculate reward and cost
        if other_questions:
            if self.enable_reward_estimate:
                other_questions = self._get_reward_estimate(other_questions)
            else:
                for q in other_questions:
                    q["reward_estimate"] = 0.0
            
            for q in other_questions:
                q["cost_estimate"] = self._get_cost_estimate(q) if self.enable_cost_estimate else 0.0
        
        # Write the updated question list back to buffer
        self.buffer.write_latest_questions(other_questions)
        
        # Rebuild DAG and calculate status
        self.buffer.build_dag()
        self.buffer.calculate_status()

        # Update priority scores
        self._update_priority_scores()
    
    
    def select_question(self):
        selected_question = self.highest_priority_question
        if selected_question is None:
            logging.info(f"[{os.getpid()}](QUE) No pending or ready questions available for selection.")
            return None
        self.buffer.set_status(selected_question["id"], "in_progress")
        return selected_question
        
        
    def answer_question(self, question):
        question_completed = self.buffer.get_question_by_id(question["id"])
        if question_completed["status"] != "completed":
            raise ValueError(f"Question {question_completed['id']} is not completed. Cannot possibly be answered.")
        self.buffer.set_answer(question_completed["id"], question["answer"])
        self.buffer.set_status(question_completed["id"], "answered")
    
    
    def add_answered_question_directly(self, question):
        """
        Directly add an answered question to the buffer, bypassing Finishing Module processing.
        This method is used to handle questions that go directly from Finishing Module to Answering Module.
        """
        question["status"] = "answered"
        question.setdefault("max_steps", -1)
        question.setdefault("used_steps", 0)
        question.setdefault("reward_estimate", 0.0)  # Add default reward estimate
        question.setdefault("cost_estimate", 0.0)    # Add default cost estimate
        question.setdefault("dependency", [])        # Add default dependency
        question.setdefault("urgency", 0.0)          # Add default urgency
        question.setdefault("scope_type", "local")   # Add default scope type
        
        # Ensure time field exists
        if "time" not in question:
            question["time"] = {}
        
        self.buffer.add_question(question)
    
    
    def is_group_completed(self, redis_conn, group_id):
        """
        Check if the current question group is fully completed: if all questions in self.buffer are in answered status,
        and the total number of questions matches the sum of "num_questions_init" and "num_questions_follow_up" in GROUP_INFO, return True; otherwise return False.
        """
        buffer_questions = self.buffer.get_buffer()
        if not buffer_questions:
            return False

        # Check all question statuses
        all_answered = all(q.get("status") == "answered" for q in buffer_questions)
        if not all_answered:
            return False

        # Get the number of questions for this group from Redis
        num_init = int(redis_conn.get(f"{GROUP_INFO['num_questions_init']}{group_id}") or 0)
        num_follow_up = int(redis_conn.get(f"{GROUP_INFO['num_questions_follow_up']}{group_id}") or 0)
        total_expected = num_init + num_follow_up

        # Check if the count matches
        if len(buffer_questions) == total_expected:
            return True
        return False
    
    
    def clear_buffer(self):
        self.buffer.clear()
        self.highest_priority_question = None
        self.highest_priority_score = float('-inf')
        logging.info(f"[{os.getpid()}](QUE) Buffer cleared.")
    
    
    def get_question_by_id(self, question_id):
        return self.buffer.get_question_by_id(question_id)
    
    
    def set_status(self, question_id, status):
        return self.buffer.set_status(question_id, status)
    
    
    def _calculate_priority_score(self, question):
        """Calculate the priority score of a question based on the joint optimization formula"""
        if not self.priority_scheduling:
            return 0.0
        
        urgency = question["urgency"] if self.enable_urgency else 0.0
        scope_type = question["scope_type"] if self.enable_scope_type else "nan"
        cost = question["cost_estimate"] if self.enable_cost_estimate else 0.0
        reward = question["reward_estimate"] if self.enable_reward_estimate else 0.0
        status = question["status"] if self.enable_dependency else "nan"
        
        scope_score = 1 if scope_type == "local" else 0
        dep_score = 1 if status == "ready" else 0
        
        # Apply joint optimization formula
        # Score_q = W_urg * (-ln(1 - Urg_q)) + W_scope * Scope_q + W_cost * Cost_q + W_reward * Reward_q + W_dep * Dep_q
        urgency_term = -math.log(1 - urgency + 1e-10)  # Add a small value to avoid log(0)
        
        score = (self.w_urgency * urgency_term + 
                self.w_scope * scope_score + 
                self.w_cost * cost + 
                self.w_reward * reward + 
                self.w_dependency * dep_score)
        
        return score
    
    
    def _update_priority_scores(self):
        """Calculate priority scores for all pending and ready questions, and find the highest scoring question"""
        questions = self.buffer.get_pending_and_ready_questions()
        
        # Reset highest priority
        self.highest_priority_question = None
        self.highest_priority_score = float('-inf')
        
        for question in questions:
            score = self._calculate_priority_score(question)
            
            # Update the highest scoring question
            if score > self.highest_priority_score:
                self.highest_priority_score = score
                self.highest_priority_question = question
                
        logging.info(f"Priority update completed, highest priority question ID: {self.highest_priority_question['id'] if self.highest_priority_question else 'None'}, score: {self.highest_priority_score}")


    def _get_reward_estimate(self, questions):
        """
        Calculate reward estimate for each question
        
        Args:
            questions: Question list
        
        Returns:
            list: Question list with updated reward_estimate
        """
        # If there's only one question, set reward directly to 1
        if len(questions) <= 1:
            for q in questions:
                q["reward_estimate"] = 1.0
            return questions
        
        # Get prompt template
        prompt_get_reward = self.prompt_updater.get("get_reward_estimate", "")
        if not prompt_get_reward:
            logging.warning("Reward estimation prompt template not found")
            # If no template, default all questions reward to 1
            for q in questions:
                q["reward_estimate"] = 1.0
            return questions
        
        # Prepare question data
        questions_data = []
        for q in questions:
            questions_data.append({
                "id": q["id"],
                "description": q["description"]
            })
        
        # Prepare formatted string of questions
        questions_str = json.dumps(questions_data, ensure_ascii=False, indent=2)
        
        # Fill prompt template
        prompt = prompt_get_reward.format(questions=questions_str)
        
        # Send request to VLM API
        response = self.vlm.request_with_retry(image=None, prompt=prompt)[0]
        
        # Parse response to get reward values
        try:
            # Find JSON format response
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                reward_data = json.loads(json_str)
                
                # Ensure correct return format
                if not isinstance(reward_data, dict):
                    logging.error(f"Reward estimation format error: {reward_data}")
                    # Default all questions reward to 1
                    for q in questions:
                        q["reward_estimate"] = 1.0
                    return questions
                    
                # Update question reward_estimate
                for q in questions:
                    q_id = q["id"]
                    if q_id in reward_data:
                        # Ensure reward is a numeric value within reasonable range
                        reward = float(reward_data[q_id])
                        reward = max(1.0, min(len(questions), reward))  # Limit between 1 and number of questions
                        q["reward_estimate"] = reward
                    else:
                        # Corresponding ID reward not found, set to default value 1
                        q["reward_estimate"] = 1.0
                        logging.warning(f"Reward value for question ID {q_id} not found, set to default value 1")
                
                return questions
            else:
                logging.error(f"Unable to extract JSON from response: {response}")
                # Default all questions reward to 1
                for q in questions:
                    q["reward_estimate"] = 1.0
                return questions
        except Exception as e:
            logging.error(f"Error parsing reward estimation: {e}, response: {response}")
            # On error, default all questions reward to 1
            for q in questions:
                q["reward_estimate"] = 1.0
            return questions
    
    
    def _get_cost_estimate(self, question):
        return 0.0
    
    
    def _get_new_dependency(self, all_questions, qid):
        """
        Generate dependency relationships for target question
        
        Args:
            all_questions: List of all questions
            qid: Target question ID
            
        Returns:
            dict: Dependency relationship dictionary containing depends_on and required_by
        """
        if not self.enable_dependency:
            return {"depends_on": [], "required_by": []}
        
        # Get prompt template
        prompt_get_dependency = self.prompt_updater.get("get_dependency", "")
        if not prompt_get_dependency:
            logging.warning("Dependency generation prompt template not found")
            return {"depends_on": [], "required_by": []}
        
        # Find target question
        target_question = None
        other_questions_data = []
        
        for q in all_questions:
            if q["id"] == qid:
                target_question = q["description"]
            else:
                other_questions_data.append({
                    "id": q["id"],
                    "description": q["description"]
                })
        
        if not target_question:
            logging.error(f"Question with ID {qid} not found in all_questions")
            return {"depends_on": [], "required_by": []}
        
        # Prepare formatted string of other questions
        other_questions_str = json.dumps(other_questions_data, ensure_ascii=False, indent=2)
        
        # Fill prompt template
        prompt = prompt_get_dependency.format(
            target_question=target_question,
            other_questions=other_questions_str
        )
        
        # Send request to VLM API
        response = self.vlm.request_with_retry(image=None, prompt=prompt)[0]
        
        # Extract JSON part from response
        try:
            # Find JSON format dependency relationships
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                dependency = json.loads(json_str)
                
                # Ensure correct return format
                if not isinstance(dependency.get("depends_on", None), list) or not isinstance(dependency.get("required_by", None), list):
                    logging.error(f"Dependency format error: {dependency}")
                    return {"depends_on": [], "required_by": []}
                    
                return dependency
            else:
                logging.error(f"Unable to extract JSON from response: {response}")
                return {"depends_on": [], "required_by": []}
        except Exception as e:
            logging.error(f"Error parsing dependency relationships: {e}, response: {response}")
            return {"depends_on": [], "required_by": []}
    
    
    def _merge_dependencies(self, all_questions, new_dependency, qid):
        """
        Merge new dependency relationships into the question list
        
        Args:
            all_questions: List of all questions
            new_dependency: New dependency relationship dictionary containing depends_on and required_by
            qid: Target question ID
            
        Returns:
            list: Updated question list
        """
        # Validate new dependency format
        if not isinstance(new_dependency, dict) or "depends_on" not in new_dependency or "required_by" not in new_dependency:
            logging.error(f"New dependency format error: {new_dependency}")
            return all_questions
        
        # Set dependencies for target question
        target_question = None
        for i, q in enumerate(all_questions):
            if q["id"] == qid:
                # Set questions that target question depends on
                all_questions[i]["dependency"] = new_dependency.get("depends_on", [])
                target_question = q
                break
        
        if not target_question:
            logging.error(f"Question with ID {qid} not found in all_questions")
            return all_questions
        
        # Update dependency relationships of other questions (add target question to dependency list of questions that need it)
        for required_by_id in new_dependency.get("required_by", []):
            for i, q in enumerate(all_questions):
                if q["id"] == required_by_id:
                    # If dependency list doesn't exist, create a new one
                    if "dependency" not in all_questions[i] or not isinstance(all_questions[i]["dependency"], list):
                        all_questions[i]["dependency"] = []
                    
                    # Add target question ID to dependency list (if not already present)
                    if qid not in all_questions[i]["dependency"]:
                        all_questions[i]["dependency"].append(qid)
                    break
        
        return all_questions
