# General
seed: 42
exp_name: ParaEQA
output_parent_dir: results
question_data_path: ./data/benchmark
scene_data_path: ./data/HM3D
init_pose_data_path: ./data/scene_init_poses_all.csv
save_obs: true
save_freq: 20
device: 'cuda'
detector: '/home/whs/eqa/para-eqa/data/yolo11x.pt' # 替换为实际的绝对路径


# Parallel
parallel:
  use_parallel: true
  enable_preempt: true
  num_init_questions: 5
  num_followup_questions: 3


# Priority
priority:
  w_urgency: 1.0
  w_scope: 1.0
  w_cost: -1.0
  w_reward: 1.0
  w_dependency: 3.0


# Redis
redis:
  host: "localhost"
  port: 6379
  db: 0


# Services
generator:
  interval_seconds: 120
  shuffle: false

finishing:
  retrieval_num: 5
  confidence_threshold: 0.7

stopping:
  retrieval_num: 5
  confidence_threshold: 0.7

updater:
  enable_cost_estimate: false
  enable_reward_estimate: false

answering:
  result_path: results/answers.json

planner:
  dist_T: 10
  unexplored_T: 0.2
  unoccupied_T: 2.0
  val_T: 0.5
  val_dir_T: 0.5
  max_val_check_frontier: 3
  smooth_sigma: 5
  eps: 1
  min_dist_from_cur: 0.5
  max_dist_from_cur: 3
  frontier_spacing: 1.5
  frontier_min_neighbors: 3
  frontier_max_neighbors: 4
  max_unexplored_check_frontier: 3
  max_unoccupied_check_frontier: 1


# ParaEQA

# Camera, image
camera_height: 1.5
camera_tilt_deg: -30
img_width: 640
img_height: 480
hfov: 120
tsdf_grid_size: 0.1
margin_w_ratio: 0.25
margin_h_ratio: 0.6

# Navigation
init_clearance: 0.5
max_step_room_size_ratio: 3
black_pixel_ratio: 0.7
min_random_init_steps: 2

# Semantic map
use_active: true
use_lsv: true
use_gsv: true
gsv_T: 0.5
gsv_F: 3

visual_prompt:
  cluster_threshold: 1.0
  num_prompt_points: 3
  num_max_unoccupied: 300
  min_points_for_clustering: 3
  point_min_dist: 2
  point_max_dist: 10
  cam_offset: 0.6
  min_num_prompt_points: 2
  circle_radius: 18


# Memory
memory:
  device: 'cuda'
  use_rag: true
  dimension: 768
  weight_image: 0.5
  weight_text: 0.5
  max_retrieval_num: 5
  replace_memory: false  # 是否替换相似的记忆
  lambda_sim: 0.5


# VLM
vlm:
  use_openrouter: true # 只有设置为 true 时，才可以使用 OpenAI 之外的模型
  device: 'cuda'
  model_api: gpt-4.1-nano
  model_local: Qwen/Qwen2-VL-2B-Instruct


# Prompts
prompt:
  planner:
    caption: f"Describe this image."
    relevent: "\nConsider the question: '{}'. How confident are you in answering this question from your current perspective?\nA. Very low\nB. Low\nC. Medium\nD. High\nE. Very high\nAnswer with the option's letter from the given choices directly."
    # relevent: "\nConsider the question: '{}'. Are you confident about answering the question with the current view? Answer with Yes or No."
    question: "{}\nAnswer with the option's letter from the given choices directly."
    # question: "{}\nAnswer with the brief sentence."
    local_sem: "\nConsider the question: '{}', and you will explore the environment for answering it.\nWhich direction (black letters on the image) would you explore then? Provide reasons and answer with a single letter."
    global_sem: "\nConsider the question: '{}', and you will explore the environment for answering it. Is there any direction shown in the image worth exploring? Answer with Yes or No."

  parser: |
    You are an AI assistant tasked with parsing natural language questions into structured data for an Embodied Question Answering (EQA) system in a multi-story residential environment. Your goal is to extract key information from a question and represent it in a JSON object containing the following fields: `urgency` and `scope_type`.

    - **urgency**: A float value between 0 and 1 indicating the urgency of the question. Safety-related questions have higher urgency (e.g., 0.9), functionality-related questions have medium urgency (e.g., 0.5), and general information questions have lower urgency (e.g., 0.2).
    - **scope_type**: An enum value indicating the scope of exploration needed. Use "local" for questions that require a single observation (e.g., checking the state of a specific object in a room) and "global" for questions that require exploring the entire environment (e.g., checking all windows across multiple floors).

    Here are some examples to guide you:

    <Example 1>
    - **Question**: "Is the window in the master bedroom on the second floor open?"
    - **Structured Output**:
    ```json
    {
        "urgency": 0.2,
        "scope_type": "local"
    }
    ```

    <Example 2>
    - **Question**: "Is the Wi-Fi router in the living room turned on?"
    - **Structured Output**:
    ```json
    {
        "urgency": 0.4,
        "scope_type": "local"
    }
    ```

    <Example 3>
    - **Question**: "I can't find my cell phone. Where can you find it?"
    - **Structured Output**:
    ```json
    {
        "urgency": 0.6,
        "scope_type": "global"
    }
    ```

    <Example 4>
    - **Question**: "Is there any fire risk in my house?"
    - **Structured Output**:
    ```json
    {
        "urgency": 0.9,
        "scope_type": "global"
    }
    ```

    Now, please parse the following question: {original_question}

    Please directly provide the structured output in JSON format and DO NOT include any additional text or explanations.
  
  finishing:
    get_confidence: example prompt
  
  stopping:
    get_confidence: example prompt
  
  answering:
    get_answer: example prompt
  
  updater:
    get_cost_estimate:
    get_reward_estimate:
    get_dependency: |
      You are an AI assistant responsible for generating a dependency graph for a set of structured questions in an Embodied Question Answering (EQA) system within a multi-story residential environment. Your task is to analyze relationships between questions and represent their dependencies in a JSON format, considering spatial, informational, and logical dependencies.

      You will be given a target question and a list of other questions. For each question in the list, you must examine carefully if it has any dependencies with the target question. That means, the target question is the precondition for the other question to be answered, or the other question is the precondition for the target question to be answered. 

      There are mainly three types of dependencies:
      1. **Spatial dependencies**: Must locate a larger space before exploring a smaller space or object (e.g., "second floor" -> "master bedroom").
      2. **Informational dependencies**: Requires another question’s answer as input (e.g., "Is the kitchen clean?" -> "Are all dishes in the sink washed?").
      3. **Logical dependencies**: Involves a cause-effect or conditional relationship (e.g., "Is the garage door closed?" -> "If the garage door is closed, is the light off?").
      All of these types of dependencies should be checked carefully, while other possible dependencies should also be considered.

      You should first think step by step about the dependencies between the target question and all other questions in the buffer, and then provide the dependencies in a strict JSON format:

      ```json
      {
        "depends_on": ["id_1", "id_2", ...],
        "required_by": ["id_3", "id_4", ...]
      }
      ```
      where:
      1. **depends_on**: the set of questions that the target question depends on. These are prerequisite questions that must be solved first before the target question can be addressed.
      2. **required_by**: the set of questions that depend on the target question. These are questions that consider the target question as a prerequisite.

      Here are some examples to guide you:

      <Example 1>

      <Example 2>

      <Example 3>

      Now, please analyze the following target question and the list of other questions, and provide the dependencies in JSON format. 

      Target question: {target_question}

      List of other questions: {other_questions}