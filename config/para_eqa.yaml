# General
seed: 42
exp_name: ParaEQA
logs_parent_dir: logs
question_data_path: ./data/benchmark
scene_data_path: ./data/HM3D
init_pose_data_path: ./data/scene_init_poses_all.csv
save_obs: true
save_freq: 20
device: 'cuda'
# TODO: 
detector: '/home/whs/eqa/para-eqa/data/yolo11x.pt' # 替换为实际的绝对路径
# TODO:
use_parallel: true # 是否有 group memory 和 finishing service
# TODO:
priority_scheduling: true
# TODO:
direct_answer: true

# NOTE: 不允许 use_parallel == True 且 use_rag == False 的情况

# Memory
memory:
  device: 'cuda'
  # TODO:
  use_rag: true # 是否有 question memory
  dimension: 768
  weight_image: 0.5
  weight_text: 0.5
  max_retrieval_num: 3
  replace_memory: false  # 是否替换相似的记忆
  lambda_sim: 0.5


# Priority
priority:
  w_urgency: 1.0
  w_scope: 1.0
  w_cost: -1.0
  w_reward: 1.0
  w_dependency: 3.0


# Redis
redis:
  host: "localhost"
  # TODO:
  port: 6379
  db: 0


# Services
generator:
  # TODO:
  enable_follow_up: true
  # TODO:
  interval_seconds: 120
  # TODO:
  start_group: 0
  end_group: 39

finishing:
  retrieval_num: 3
  confidence_threshold: 0.5
  enable_tryout_answer: false

stopping:
  retrieval_num: 3
  confidence_threshold: 0.5
  enable_tryout_answer: false

updater:
  enable_cost_estimate: false
  # TODO:
  enable_reward_estimate: true

answering:
  # TODO:
  result_dir: results/answers-test

planner:
  dist_T: 10
  unexplored_T: 0.2
  unoccupied_T: 2.0
  val_T: 0.5
  val_dir_T: 0.5
  max_val_check_frontier: 3
  smooth_sigma: 5
  eps: 1
  min_dist_from_cur: 0.5
  max_dist_from_cur: 3
  frontier_spacing: 1.5
  frontier_min_neighbors: 3
  frontier_max_neighbors: 4
  max_unexplored_check_frontier: 3
  max_unoccupied_check_frontier: 1


# ParaEQA

# Camera, image
camera_height: 1.5
camera_tilt_deg: -30
img_width: 640
img_height: 480
hfov: 120
tsdf_grid_size: 0.1
margin_w_ratio: 0.25
margin_h_ratio: 0.6

# Navigation
init_clearance: 0.5
max_step_room_size_ratio: 3
black_pixel_ratio: 0.7
min_random_init_steps: 2

# Semantic map
use_active: true
use_lsv: true
use_gsv: true
gsv_T: 0.5
gsv_F: 3

visual_prompt:
  cluster_threshold: 1.0
  num_prompt_points: 3
  num_max_unoccupied: 300
  min_points_for_clustering: 3
  point_min_dist: 2
  point_max_dist: 10
  cam_offset: 0.6
  min_num_prompt_points: 2
  circle_radius: 18


# VLM
vlm:
  device: 'cuda'

  # chose server from {"openai", "openrouter", "dashscope", "ollama", "other"}
  # RTX 5090 * 1: "http://100.115.177.101:11434/v1"
  # RTX 5090 * 1 (1): "http://100.81.140.32:11434/v1"
  # RTX 5090 * 1 (2): "http://100.88.238.80:11434/v1"
  # RTX 4090 * 4: "http://58.198.176.200:11434/v1"
  # RTX 4090 D * 1: "http://49.52.18.227:11434/v1"

  # 稳定高成本
  answering:
    model: openai/gpt-5-mini
    server: openrouter
    base_url: 
    api_key:

  finishing:
    model: openai/gpt-5-mini
    server: openrouter
    base_url: 
    api_key:

  parser:
    model: openai/gpt-oss-20b
    server: openrouter
    base_url: 
    api_key:

  stopping:
    model: openai/gpt-5-mini
    server: openrouter
    base_url: 
    api_key:

  updater:
    model: openai/gpt-oss-20b
    server: openrouter
    base_url: 
    api_key:

  planner:
    model: qwen2.5-vl-72b-instruct
    server: dashscope
    base_url: 
    api_key:

  planner_lite:
    model: qwen2.5-vl-72b-instruct
    server: dashscope
    base_url: 
    api_key:

  planner_tiny:
    model: qwen2.5-vl-7b-instruct
    server: dashscope
    base_url: 
    api_key:

  # # 适中成本
  # answering:
  #   model: openai/gpt-5-mini
  #   server: openrouter
  #   base_url: 
  #   api_key:

  # finishing:
  #   model: openai/gpt-5-mini
  #   server: openrouter
  #   base_url: 
  #   api_key:

  # parser:
  #   model: gpt-oss:20b
  #   server: ollama
  #   base_url: "http://100.81.140.32:11434/v1"
  #   api_key:

  # stopping:
  #   model: openai/gpt-5-mini
  #   server: openrouter
  #   base_url: 
  #   api_key:

  # updater:
  #   model: gpt-oss:20b
  #   server: ollama
  #   base_url: "http://100.81.140.32:11434/v1"
  #   api_key:

  # planner:
  #   model: qwen2.5vl:72b
  #   server: ollama
  #   base_url: "http://58.198.176.200:11434/v1"
  #   api_key:

  # planner_lite:
  #   model: qwen2.5vl:72b
  #   server: ollama
  #   base_url: "http://58.198.176.200:11434/v1"
  #   api_key:

  # planner_tiny:
  #   model: qwen2.5vl:7b
  #   server: ollama
  #   base_url: "http://100.88.238.80:11434/v1"
  #   api_key:
  
  # 零成本高性能
  # answering:
  #   model: qwen2.5vl:72b
  #   server: ollama
  #   base_url: "http://58.198.176.200:11434/v1"
  #   api_key:

  # finishing:
  #   model: qwen2.5vl:72b
  #   server: ollama
  #   base_url: "http://58.198.176.200:11434/v1"
  #   api_key:

  # parser:
  #   model: gpt-oss:20b
  #   server: ollama
  #   base_url: "http://100.81.140.32:11434/v1"
  #   api_key:

  # stopping:
  #   model: qwen2.5vl:72b
  #   server: ollama
  #   base_url: "http://58.198.176.200:11434/v1"
  #   api_key:

  # updater:
  #   model: gpt-oss:20b
  #   server: ollama
  #   base_url: "http://100.81.140.32:11434/v1"
  #   api_key:

  # planner:
  #   model: qwen2.5vl:72b
  #   server: ollama
  #   base_url: "http://58.198.176.200:11434/v1"
  #   api_key:

  # planner_lite:
  #   model: qwen2.5vl:72b
  #   server: ollama
  #   base_url: "http://58.198.176.200:11434/v1"
  #   api_key:

  # planner_tiny:
  #   model: qwen2.5vl:7b
  #   server: ollama
  #   base_url: "http://100.88.238.80:11434/v1"
  #   api_key:


# Prompts
prompt:
  planner:
    caption: "Describe this image."
    relevent: "\nConsider the question: '{}'. How confident are you in answering this question from your current perspective?\nA. Very low\nB. Low\nC. Medium\nD. High\nE. Very high\nAnswer with the option's letter from the given choices directly."
    question: "{}\nAnswer with the option's letter from the given choices directly."
    local_sem: "\nConsider the question: '{}', and you will explore the environment for answering it.\nWhich direction (black letters on the image) would you explore then? Provide reasons and answer with a single letter."
    global_sem: "\nConsider the question: '{}', and you will explore the environment for answering it. Is there any direction shown in the image worth exploring? Answer with Yes or No."

  parser: |
    You are an AI assistant tasked with parsing natural language questions into structured data for an Embodied Question Answering (EQA) system in a multi-story residential environment. Your goal is to extract key information from a question and represent it in a JSON object containing the following fields: `urgency` and `scope_type`.

    - **urgency**: A float value between 0 and 1 indicating the urgency of the question. Safety-related questions have higher urgency (e.g., 0.9), functionality-related questions have medium urgency (e.g., 0.5), and general information questions have lower urgency (e.g., 0.2).
    - **scope_type**: An enum value indicating the scope of exploration needed. Use "local" for questions that require a single observation (e.g., checking the state of a specific object in a room) and "global" for questions that require exploring the entire environment (e.g., checking all windows across multiple floors).

    Here are some examples to guide you:

    <Example 1>
    - **Question**: "What color is the sofa in the living room?"
    - **Structured Output**:
    ```json
    {
        "urgency": 0.2,
        "scope_type": "local"
    }
    ```

    <Example 2>
    - **Question**: "Is the TV in the living room turned on?"
    - **Structured Output**:
    ```json
    {
        "urgency": 0.4,
        "scope_type": "local"
    }
    ```

    <Example 3>
    - **Question**: "Where can you find my cell phone?"
    - **Structured Output**:
    ```json
    {
        "urgency": 0.6,
        "scope_type": "global"
    }
    ```

    <Example 4>
    - **Question**: "Is there any fire risk in my house?"
    - **Structured Output**:
    ```json
    {
        "urgency": 0.9,
        "scope_type": "global"
    }
    ```

    Now, please parse the following question: {original_question}

    Please directly provide the structured output in JSON format and DO NOT include any additional text or explanations.
  
  finishing:
    get_tryout_answer: |
      Consider the question: '{}'.
      Answer with the option's letter from the given choices directly based on the information currently available to you (may include the knowledge base, images and context infomation).
      If you have no idea what the correct answer is, just choose an answer randomly. DO NOT include any additional text or explanations. Also DO NOT include any option's text in your answer.
    get_tryout_confidence: |
      Consider the question: '{question_desc}'.
      If the proposed answer is '{tryout_answer}', how confident are you in confirming this answer is correct based on the information available in the knowledge base?

      A. Very low (Almost no evidence from the knowledge base, and the answer clearly conflicts with known information)
      B. Low (Very little or unclear evidence from the knowledge base, and most of the answer is inconsistent with available information)
      C. Medium (Some relevant evidence in the knowledge base supports the answer, but the evidence is incomplete or partially inconsistent)
      D. High (The knowledge base largely supports the answer, with only minor uncertainty or missing details)
      E. Very high (The knowledge base fully supports the answer, with complete and unambiguous confirmation)

      Answer with the option's letter ("A", "B", "C", "D", or "E") from the given choices directly. DO NOT include any additional text or explanations. Also DO NOT include any option's text in your answer.

    get_confidence: |
      Consider the question: '{}'.
      If you were asked to answer this question right now, how confident are you that you can provide a correct answer based on the information currently available to you (may include the knowledge base, images and context infomation)?

      A. Very low (Almost no supporting evidence exists in the knowledge base or context; the answer is unsupported or likely to be wrong)
      B. Low (Only very limited, vague, or indirect support in the knowledge base; significant parts of the answer would require guessing or unsupported assumptions)
      C. Medium (Some relevant evidence exists in the knowledge base or context that points toward the answer, but the evidence is incomplete, partially inconsistent, or requires moderate inference)
      D. High (Clear and substantial evidence in the knowledge base and context supports the answer; only minor details are uncertain or missing)
      E. Very high (Complete, direct, and unambiguous support in the knowledge base and context; you can answer correctly now with high confidence)

      Answer with the single option letter ("A", "B", "C", "D", or "E") only. DO NOT include any additional text, explanation, or the option descriptions. Use an uppercase letter.

  
  stopping:
    get_tryout_answer: |
      Consider the question: '{}'.
      Answer with the option's letter from the given choices directly based on the information currently available to you (may include the knowledge base, images and context infomation).
      If you have no idea what the correct answer is, just choose an answer randomly. DO NOT include any additional text or explanations. Also DO NOT include any option's text in your answer.
    get_tryout_confidence: |
      Consider the question: '{question_desc}'.
      If the proposed answer is '{tryout_answer}', how confident are you in confirming this answer is correct based on the information available in the knowledge base?

      A. Very low (Almost no evidence from the knowledge base, and the answer clearly conflicts with known information)
      B. Low (Very little or unclear evidence from the knowledge base, and most of the answer is inconsistent with available information)
      C. Medium (Some relevant evidence in the knowledge base supports the answer, but the evidence is incomplete or partially inconsistent)
      D. High (The knowledge base largely supports the answer, with only minor uncertainty or missing details)
      E. Very high (The knowledge base fully supports the answer, with complete and unambiguous confirmation)

      Answer with the option's letter ("A", "B", "C", "D", or "E") from the given choices directly. DO NOT include any additional text or explanations. Also DO NOT include any option's text in your answer.
    get_confidence: |
      Consider the question: '{}'.
      If you were asked to answer this question right now, how confident are you that you can provide a correct answer based on the information currently available to you (may include the knowledge base, images and context infomation)?

      A. Very low (Almost no supporting evidence exists in the knowledge base or context; the answer is unsupported or likely to be wrong)
      B. Low (Only very limited, vague, or indirect support in the knowledge base; significant parts of the answer would require guessing or unsupported assumptions)
      C. Medium (Some relevant evidence exists in the knowledge base or context that points toward the answer, but the evidence is incomplete, partially inconsistent, or requires moderate inference)
      D. High (Clear and substantial evidence in the knowledge base and context supports the answer; only minor details are uncertain or missing)
      E. Very high (Complete, direct, and unambiguous support in the knowledge base and context; you can answer correctly now with high confidence)

      Answer with the single option letter ("A", "B", "C", "D", or "E") only. DO NOT include any additional text, explanation, or the option descriptions. Use an uppercase letter.
  
  answering:
    get_answer: |
      Consider the question: '{}'.
      Answer with the option's letter from the given choices directly based on the information currently available to you (may include the knowledge base, images and context infomation).
      If you have no idea what the correct answer is, just choose an answer randomly. DO NOT include any additional text or explanations. Also DO NOT include any option's text in your answer.
  
  updater:
    get_cost_estimate:
    get_reward_estimate: |
      You are an AI assistant responsible for estimating reward values for a set of questions in an Embodied Question Answering (EQA) system. Your task is to analyze the spatial relationships between questions and assign appropriate reward values.

      The reward value for each question is determined by the number of questions that refer to the same or nearby locations or objects. If multiple questions target the same location or object, they should all receive a reward equal to the count of such questions. If a question targets a unique location or object, its reward is 1.

      Important: Be careful to differentiate between similar locations in different areas. For example, "bedroom on the first floor" and "bedroom on the second floor" are different locations, while "bed in the master bedroom" and "nightstand in the master bedroom" are in the same location.

      Here are some examples:

      Example 1:
      ```
      [
        {{
          "id": "q1",
          "description": "What is the color of the kitchen countertop?"
        }}
      ]
      ```
      Reward Analysis: Only one question, so reward = 1.
      Output:
      ```json
      {{
        "q1": 1
      }}
      ```

      Example 2:
      ```
      [
        {{
          "id": "q1",
          "description": "How many white plates are there on the dining table?"
        }},
        {{
          "id": "q2",
          "description": "What is the dining table made of?"
        }}
      ]
      ```
      Reward Analysis: Both questions refer to the dining table, so reward = 2 for each.
      Output:
      ```json
      {{
        "q1": 2,
        "q2": 2
      }}
      ```

      Example 3:
      ```
      [
        {{
          "id": "q1", 
          "description": "Is there a vase on the bedroom floor?"
        }},
        {{
          "id": "q2",
          "description": "How many pillows are on the bedroom bed?"
        }},
        {{
          "id": "q3",
          "description": "What color is the blanket on the bedroom bed?"
        }}
      ]
      ```
      Reward Analysis: All three questions refer to the same bedroom, so reward = 3 for each.
      Output:
      ```json
      {{
        "q1": 3,
        "q2": 3,
        "q3": 3
      }}
      ```

      Example 4:
      ```
      [
        {{
          "id": "q1",
          "description": "What color is the sofa in the living room?"
        }},
        {{
          "id": "q2",
          "description": "Is there a TV in the master bedroom?"
        }},
        {{
          "id": "q3",
          "description": "How many windows are in the living room?"
        }},
        {{
          "id": "q4",
          "description": "Is there a desk in the guest bedroom?"
        }}
      ]
      ```
      Reward Analysis: q1 and q3 refer to the living room (reward = 2), q2 refers to the master bedroom (reward = 1), q4 refers to the guest bedroom (reward = 1).
      Output:
      ```json
      {{
        "q1": 2,
        "q2": 1,
        "q3": 2,
        "q4": 1
      }}
      ```

      Now, please analyze the following questions and provide the reward estimates in JSON format with question IDs as keys and reward values as integers. The output should ONLY contain the JSON object. DO NOT include any additional text or explanations.

      Questions: {questions}
    get_dependency: |
      You are an AI assistant responsible for generating a dependency graph for a set of structured questions in an Embodied Question Answering (EQA) system within a multi-story residential environment. Your task is to analyze relationships between questions and represent their dependencies in a JSON format, considering spatial, informational, and logical dependencies.

      You will be given a target question and a list of other questions. For each question in the list, you must examine carefully if it has any dependencies with the target question. That means, the target question is the precondition for the other question to be answered, or the other question is the precondition for the target question to be answered. 

      There are mainly three types of dependencies:
      1. **Spatial dependencies**: Must locate a larger space before exploring a smaller space or object (e.g., "second floor" -> "master bedroom").
      2. **Informational dependencies**: Requires another question’s answer as input (e.g., "Is the kitchen clean?" -> "Are all dishes in the sink washed?").
      3. **Logical dependencies**: Involves a cause-effect or conditional relationship (e.g., "Is the garage door closed?" -> "If the garage door is closed, is the light off?").
      All of these types of dependencies should be checked carefully, while other possible dependencies should also be considered.

      You should first think step by step about the dependencies between the target question and all other questions in the buffer, and then provide the dependencies in a strict JSON format:

      ```json
      {{
        "depends_on": ["id_1", "id_2", ...],
        "required_by": ["id_3", "id_4", ...]
      }}
      ```
      where:
      1. **depends_on**: the set of questions that the target question depends on. These are prerequisite questions that must be solved first before the target question can be addressed.
      2. **required_by**: the set of questions that depend on the target question. These are questions that consider the target question as a prerequisite.

      Now, please analyze the following target question and the list of other questions, and provide the dependencies in JSON format. NOTE that your response should only contain the JSON object. DO NOT include any additional text or explanations.

      Target question: {target_question}

      List of other questions: {other_questions}